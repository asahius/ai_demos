{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKu2--YjjiNd"
      },
      "source": [
        "論文  \n",
        "https://arxiv.org/abs/2112.10741<br>\n",
        "GitHub<br>\n",
        "https://github.com/openai/glide-text2im<br>\n",
        "<br>\n",
        "<a href=\"https://colab.research.google.com/github/kaz12tech/ai_demos/blob/master/GLIDE_demo.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWhJj2RejiNg"
      },
      "source": [
        "# 環境セットアップ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IuH74fQjiNg"
      },
      "source": [
        "## GPU確認"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PcII5fLdHnJ"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pEp-7bNjiNj"
      },
      "source": [
        "## ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LScsvO3UppqT"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "!pip install git+https://github.com/openai/glide-text2im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDsSRIjsjiNj"
      },
      "source": [
        "## ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqJzYJTmEI_I"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "import base64\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "\n",
        "from glide_text2im.clip.model_creation import create_clip_model\n",
        "from glide_text2im.download import load_checkpoint\n",
        "from glide_text2im.model_creation import (\n",
        "    create_model_and_diffusion,\n",
        "    model_and_diffusion_defaults,\n",
        "    model_and_diffusion_defaults_upsampler\n",
        ")\n",
        "from glide_text2im.tokenizer.simple_tokenizer import SimpleTokenizer\n",
        "\n",
        "has_cuda = th.cuda.is_available()\n",
        "device = th.device('cpu' if not has_cuda else 'cuda')\n",
        "print(\"device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TithP1GkS2L"
      },
      "source": [
        "# 入出力関数定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDabatfZkU0y"
      },
      "outputs": [],
      "source": [
        "def show_images(batch: th.Tensor):\n",
        "    \"\"\" Display a batch of images inline. \"\"\"\n",
        "    scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n",
        "    reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n",
        "    display(Image.fromarray(reshaped.numpy()))\n",
        "\n",
        "def read_image(path: str, size: int = 256) -> Tuple[th.Tensor, th.Tensor]:\n",
        "    pil_img = Image.open(path).convert('RGB')\n",
        "    pil_img = pil_img.resize((size, size), resample=Image.BICUBIC)\n",
        "    img = np.array(pil_img)\n",
        "    print(img.shape)\n",
        "    return th.from_numpy(img)[None].permute(0, 3, 1, 2).float() / 127.5 - 1\n",
        "\n",
        "def read_image_mask(path: str, size: int = 256) -> Tuple[th.Tensor, th.Tensor]:\n",
        "    pil_img = Image.open(path).convert(mode=\"L\")\n",
        "    pil_img = pil_img.resize((size, size), resample=Image.BICUBIC)\n",
        "    img = np.array(pil_img)\n",
        "    img = np.where(img != 255, 0, img)\n",
        "    img = np.where(img == 255, 1, img)\n",
        "    th_img = th.from_numpy(img)\n",
        "    th_img = th.unsqueeze(th_img, 0)\n",
        "    th_img = th.unsqueeze(th_img, 0)\n",
        "    return th_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NPtkh-1lYYP"
      },
      "source": [
        "# Text to Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU0Tiy2XkubN"
      },
      "source": [
        "## text設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLfSaGwMkt-1"
      },
      "outputs": [],
      "source": [
        "# Sampling parameters\n",
        "prompt = \"standing panda\" # @param {type:\"string\"} \n",
        "batch_size = 1\n",
        "guidance_scale = 3.0\n",
        "\n",
        "# Tune this parameter to control the sharpness of 256x256 images.\n",
        "# A value of 1.0 is sharper, but sometimes results in grainy artifacts.\n",
        "upsample_temp = 0.997"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAKEbnkPzULg"
      },
      "source": [
        "## Model作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHiLQ-nDiVgd"
      },
      "outputs": [],
      "source": [
        "options = model_and_diffusion_defaults()\n",
        "options['use_fp16'] = has_cuda\n",
        "\n",
        "# diffusion steps\n",
        "options['timestep_respacing'] = '100'\n",
        "\n",
        "model, diffusion = create_model_and_diffusion(**options)\n",
        "model.eval()\n",
        "if has_cuda:\n",
        "    model.convert_to_fp16()\n",
        "model.to(device)\n",
        "model.load_state_dict(load_checkpoint('base', device))\n",
        "print('total base parameters', sum(x.numel() for x in model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NGh0ON2zX6x"
      },
      "source": [
        "## Upsamplerモデル作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLPYGzYOkJ7p"
      },
      "outputs": [],
      "source": [
        "options_up = model_and_diffusion_defaults_upsampler()\n",
        "options_up['use_fp16'] = has_cuda\n",
        "options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n",
        "model_up, diffusion_up = create_model_and_diffusion(**options_up)\n",
        "model_up.eval()\n",
        "if has_cuda:\n",
        "    model_up.convert_to_fp16()\n",
        "model_up.to(device)\n",
        "model_up.load_state_dict(load_checkpoint('upsample', device))\n",
        "print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj_f45FczdPp"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7_P9YhQlR-B"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "# Sample from the base model #\n",
        "##############################\n",
        "\n",
        "# Create the text tokens to feed to the model.\n",
        "tokens = model.tokenizer.encode(prompt)\n",
        "tokens, mask = model.tokenizer.padded_tokens_and_mask(\n",
        "    tokens, options['text_ctx']\n",
        ")\n",
        "\n",
        "# Create the classifier-free guidance tokens (empty)\n",
        "full_batch_size = batch_size * 2\n",
        "uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n",
        "    [], options['text_ctx']\n",
        ")\n",
        "\n",
        "# Pack the tokens together into model kwargs.\n",
        "model_kwargs = dict(\n",
        "    tokens=th.tensor(\n",
        "        [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n",
        "    ),\n",
        "    mask=th.tensor(\n",
        "        [mask] * batch_size + [uncond_mask] * batch_size,\n",
        "        dtype=th.bool,\n",
        "        device=device,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Create a classifier-free guidance sampling function\n",
        "def model_fn(x_t, ts, **kwargs):\n",
        "    half = x_t[: len(x_t) // 2]\n",
        "    combined = th.cat([half, half], dim=0)\n",
        "    model_out = model(combined, ts, **kwargs)\n",
        "    eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "    cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
        "    half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
        "    eps = th.cat([half_eps, half_eps], dim=0)\n",
        "    return th.cat([eps, rest], dim=1)\n",
        "\n",
        "# Sample from the base model.\n",
        "model.del_cache()\n",
        "samples = diffusion.p_sample_loop(\n",
        "    model_fn,\n",
        "    (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n",
        "    device=device,\n",
        "    clip_denoised=True,\n",
        "    progress=True,\n",
        "    model_kwargs=model_kwargs,\n",
        "    cond_fn=None,\n",
        ")[:batch_size]\n",
        "model.del_cache()\n",
        "\n",
        "# Show the output\n",
        "show_images(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJdCF9IwlcAM"
      },
      "source": [
        "## Upsampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "568DK01zlW2-"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "# Upsample the 64x64 samples #\n",
        "##############################\n",
        "\n",
        "tokens = model_up.tokenizer.encode(prompt)\n",
        "tokens, mask = model_up.tokenizer.padded_tokens_and_mask(\n",
        "    tokens, options_up['text_ctx']\n",
        ")\n",
        "\n",
        "# Create the model conditioning dict.\n",
        "model_kwargs = dict(\n",
        "    # Low-res image to upsample.\n",
        "    low_res=((samples+1)*127.5).round()/127.5 - 1,\n",
        "\n",
        "    # Text tokens\n",
        "    tokens=th.tensor(\n",
        "        [tokens] * batch_size, device=device\n",
        "    ),\n",
        "    mask=th.tensor(\n",
        "        [mask] * batch_size,\n",
        "        dtype=th.bool,\n",
        "        device=device,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Sample from the base model.\n",
        "model_up.del_cache()\n",
        "up_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\n",
        "up_samples = diffusion_up.ddim_sample_loop(\n",
        "    model_up,\n",
        "    up_shape,\n",
        "    noise=th.randn(up_shape, device=device) * upsample_temp,\n",
        "    device=device,\n",
        "    clip_denoised=True,\n",
        "    progress=True,\n",
        "    model_kwargs=model_kwargs,\n",
        "    cond_fn=None,\n",
        ")[:batch_size]\n",
        "model_up.del_cache()\n",
        "\n",
        "# Show the output\n",
        "show_images(up_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7SaqHWrrdzl"
      },
      "source": [
        "# Inpainting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGvaw9eszhNs"
      },
      "source": [
        "## 入力画像取得\n",
        "[使用画像](https://www.pakutaso.com/shared/img/thumb/tsj86_cafedenojk20150208114118_TP_V4.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQx2wpdmupQX"
      },
      "outputs": [],
      "source": [
        "img_path = \"test.png\"\n",
        "mask_path = \"test_mask.png\"\n",
        "\n",
        "!wget -c https://www.pakutaso.com/shared/img/thumb/tsj86_cafedenojk20150208114118_TP_V4.jpg \\\n",
        "      -O {img_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 修復範囲指定"
      ],
      "metadata": {
        "id": "VK4janaT-TSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "canvas_html = \"\"\"\n",
        "<style>\n",
        ".button {\n",
        "  background-color: #4CAF50;\n",
        "  border: none;\n",
        "  color: white;\n",
        "  padding: 15px 32px;\n",
        "  text-align: center;\n",
        "  text-decoration: none;\n",
        "  display: inline-block;\n",
        "  font-size: 16px;\n",
        "  margin: 4px 2px;\n",
        "  cursor: pointer;\n",
        "}\n",
        "</style>\n",
        "<canvas1 width=%d height=%d>\n",
        "</canvas1>\n",
        "<canvas width=%d height=%d>\n",
        "</canvas>\n",
        "\n",
        "<button class=\"button\">Finish</button>\n",
        "<script>\n",
        "var canvas = document.querySelector('canvas')\n",
        "var ctx = canvas.getContext('2d')\n",
        "\n",
        "var canvas1 = document.querySelector('canvas1')\n",
        "var ctx1 = canvas.getContext('2d')\n",
        "\n",
        "\n",
        "ctx.strokeStyle = 'red';\n",
        "\n",
        "var img = new Image();\n",
        "img.src = \"data:image/%s;charset=utf-8;base64,%s\";\n",
        "console.log(img)\n",
        "img.onload = function() {\n",
        "  ctx1.drawImage(img, 0, 0);\n",
        "};\n",
        "img.crossOrigin = 'Anonymous';\n",
        "\n",
        "ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "\n",
        "ctx.lineWidth = %d\n",
        "var button = document.querySelector('button')\n",
        "var mouse = {x: 0, y: 0}\n",
        "\n",
        "canvas.addEventListener('mousemove', function(e) {\n",
        "  mouse.x = e.pageX - this.offsetLeft\n",
        "  mouse.y = e.pageY - this.offsetTop\n",
        "})\n",
        "canvas.onmousedown = ()=>{\n",
        "  ctx.beginPath()\n",
        "  ctx.moveTo(mouse.x, mouse.y)\n",
        "  canvas.addEventListener('mousemove', onPaint)\n",
        "}\n",
        "canvas.onmouseup = ()=>{\n",
        "  canvas.removeEventListener('mousemove', onPaint)\n",
        "}\n",
        "var onPaint = ()=>{\n",
        "  ctx.lineTo(mouse.x, mouse.y)\n",
        "  ctx.stroke()\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "  button.onclick = ()=>{\n",
        "    resolve(canvas.toDataURL('image/png'))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def draw(imgm, filename='drawing.png', w=400, h=200, line_width=1):\n",
        "  display(HTML(canvas_html % (w, h, w,h, filename.split('.')[-1], imgm, line_width)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = base64.b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)"
      ],
      "metadata": {
        "id": "DDt6yNJS-SPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = np.array(plt.imread(f'{img_path}',0)[:,:,:3])\n",
        "image64 = base64.b64encode(open(img_path, 'rb').read())\n",
        "image64 = image64.decode('utf-8')\n",
        "draw(image64, filename=mask_path, w=img.shape[1], h=img.shape[0], line_width=0.04*img.shape[1])"
      ],
      "metadata": {
        "id": "zg3sNk0L_ANh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_mask = np.array(plt.imread(mask_path)[:,:,:3])\n",
        "mask = (with_mask[:,:,0]==1)*(with_mask[:,:,1]==0)*(with_mask[:,:,2]==0)\n",
        "plt.imsave(mask_path,mask, cmap='gray')\n",
        "\n",
        "im = Image.open(mask_path).convert('RGB')\n",
        "im_invert = ImageOps.invert(im)\n",
        "im_invert.save(mask_path)"
      ],
      "metadata": {
        "id": "BJELYY78I4zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFJ_dTTe13Qj"
      },
      "source": [
        "# text設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0crhwX7155u"
      },
      "outputs": [],
      "source": [
        "# Sampling parameters\n",
        "prompt = \"Cute corgi face\"\n",
        "batch_size = 1\n",
        "guidance_scale = 5.0\n",
        "\n",
        "# Tune this parameter to control the sharpness of 256x256 images.\n",
        "# A value of 1.0 is sharper, but sometimes results in grainy artifacts.\n",
        "upsample_temp = 0.997"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBJ8t2SozkdA"
      },
      "source": [
        "## Model作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhBWGPmjvgyQ"
      },
      "outputs": [],
      "source": [
        "# build inpaint model\n",
        "options = model_and_diffusion_defaults()\n",
        "options['inpaint'] = True\n",
        "options['use_fp16'] = has_cuda\n",
        "options['timestep_respacing'] = '100' # use 100 diffusion steps for fast sampling\n",
        "model, diffusion = create_model_and_diffusion(**options)\n",
        "model.eval()\n",
        "if has_cuda:\n",
        "    model.convert_to_fp16()\n",
        "model.to(device)\n",
        "model.load_state_dict(load_checkpoint('base-inpaint', device))\n",
        "print('total base parameters', sum(x.numel() for x in model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kku4rRFznUy"
      },
      "source": [
        "## Upsampleモデル作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHJvwnWDvzMt"
      },
      "outputs": [],
      "source": [
        "# Create upsampler model.\n",
        "options_up = model_and_diffusion_defaults_upsampler()\n",
        "options_up['inpaint'] = True\n",
        "options_up['use_fp16'] = has_cuda\n",
        "options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n",
        "model_up, diffusion_up = create_model_and_diffusion(**options_up)\n",
        "model_up.eval()\n",
        "if has_cuda:\n",
        "    model_up.convert_to_fp16()\n",
        "model_up.to(device)\n",
        "model_up.load_state_dict(load_checkpoint('upsample-inpaint', device))\n",
        "print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uANMuhGhzqkC"
      },
      "source": [
        "## Inpainting画像生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVn62ccVrqPe"
      },
      "outputs": [],
      "source": [
        "# Source image we are inpainting\n",
        "source_image_256 = read_image(img_path, size=256)\n",
        "source_image_64 = read_image(img_path, size=64)\n",
        "\n",
        "# The mask should always be a boolean 64x64 mask, and then we\n",
        "# can upsample it for the second stage.\n",
        "# source_mask_64 = th.ones_like(source_image_64)[:, :1]\n",
        "# source_mask_64[:, :, 20:] = 0\n",
        "# source_mask_256 = F.interpolate(source_mask_64, (256, 256), mode='nearest')\n",
        "\n",
        "source_mask_64 = read_image_mask(mask_path, size=64)\n",
        "source_mask_256 = F.interpolate(source_mask_64, (256, 256), mode='nearest')\n",
        "\n",
        "\n",
        "# Visualize the image we are inpainting\n",
        "show_images(source_image_256)\n",
        "show_images(source_image_256 * source_mask_256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOOhVzSLz0D8"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7hS-4SHrwwF"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "# Sample from the base model #\n",
        "##############################\n",
        "\n",
        "# Create the text tokens to feed to the model.\n",
        "tokens = model.tokenizer.encode(prompt)\n",
        "tokens, mask = model.tokenizer.padded_tokens_and_mask(\n",
        "    tokens, options['text_ctx']\n",
        ")\n",
        "\n",
        "# Create the classifier-free guidance tokens (empty)\n",
        "full_batch_size = batch_size * 2\n",
        "uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n",
        "    [], options['text_ctx']\n",
        ")\n",
        "\n",
        "# Pack the tokens together into model kwargs.\n",
        "model_kwargs = dict(\n",
        "    tokens=th.tensor(\n",
        "        [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n",
        "    ),\n",
        "    mask=th.tensor(\n",
        "        [mask] * batch_size + [uncond_mask] * batch_size,\n",
        "        dtype=th.bool,\n",
        "        device=device,\n",
        "    ),\n",
        "\n",
        "    # Masked inpainting image\n",
        "    inpaint_image=(source_image_64 * source_mask_64).repeat(full_batch_size, 1, 1, 1).to(device),\n",
        "    inpaint_mask=source_mask_64.repeat(full_batch_size, 1, 1, 1).to(device),\n",
        ")\n",
        "\n",
        "# Create an classifier-free guidance sampling function\n",
        "def model_fn(x_t, ts, **kwargs):\n",
        "    half = x_t[: len(x_t) // 2]\n",
        "    combined = th.cat([half, half], dim=0)\n",
        "    model_out = model(combined, ts, **kwargs)\n",
        "    eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "    cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
        "    half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
        "    eps = th.cat([half_eps, half_eps], dim=0)\n",
        "    return th.cat([eps, rest], dim=1)\n",
        "\n",
        "def denoised_fn(x_start):\n",
        "    # Force the model to have the exact right x_start predictions\n",
        "    # for the part of the image which is known.\n",
        "    return (\n",
        "        x_start * (1 - model_kwargs['inpaint_mask'])\n",
        "        + model_kwargs['inpaint_image'] * model_kwargs['inpaint_mask']\n",
        "    )\n",
        "\n",
        "# Sample from the base model.\n",
        "model.del_cache()\n",
        "samples = diffusion.p_sample_loop(\n",
        "    model_fn,\n",
        "    (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n",
        "    device=device,\n",
        "    clip_denoised=True,\n",
        "    progress=True,\n",
        "    model_kwargs=model_kwargs,\n",
        "    cond_fn=None,\n",
        "    denoised_fn=denoised_fn,\n",
        ")[:batch_size]\n",
        "model.del_cache()\n",
        "\n",
        "# Show the output\n",
        "show_images(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvMfkBYar8rr"
      },
      "source": [
        "## Upsampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pFXXWgir-0z"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "# Upsample the 64x64 samples #\n",
        "##############################\n",
        "\n",
        "tokens = model_up.tokenizer.encode(prompt)\n",
        "tokens, mask = model_up.tokenizer.padded_tokens_and_mask(\n",
        "    tokens, options_up['text_ctx']\n",
        ")\n",
        "\n",
        "# Create the model conditioning dict.\n",
        "model_kwargs = dict(\n",
        "    # Low-res image to upsample.\n",
        "    low_res=((samples+1)*127.5).round()/127.5 - 1,\n",
        "\n",
        "    # Text tokens\n",
        "    tokens=th.tensor(\n",
        "        [tokens] * batch_size, device=device\n",
        "    ),\n",
        "    mask=th.tensor(\n",
        "        [mask] * batch_size,\n",
        "        dtype=th.bool,\n",
        "        device=device,\n",
        "    ),\n",
        "\n",
        "    # Masked inpainting image.\n",
        "    inpaint_image=(source_image_256 * source_mask_256).repeat(batch_size, 1, 1, 1).to(device),\n",
        "    inpaint_mask=source_mask_256.repeat(batch_size, 1, 1, 1).to(device),\n",
        ")\n",
        "\n",
        "def denoised_fn(x_start):\n",
        "    # Force the model to have the exact right x_start predictions\n",
        "    # for the part of the image which is known.\n",
        "    return (\n",
        "        x_start * (1 - model_kwargs['inpaint_mask'])\n",
        "        + model_kwargs['inpaint_image'] * model_kwargs['inpaint_mask']\n",
        "    )\n",
        "\n",
        "# Sample from the base model.\n",
        "model_up.del_cache()\n",
        "up_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\n",
        "up_samples = diffusion_up.p_sample_loop(\n",
        "    model_up,\n",
        "    up_shape,\n",
        "    noise=th.randn(up_shape, device=device) * upsample_temp,\n",
        "    device=device,\n",
        "    clip_denoised=True,\n",
        "    progress=True,\n",
        "    model_kwargs=model_kwargs,\n",
        "    cond_fn=None,\n",
        "    denoised_fn=denoised_fn,\n",
        ")[:batch_size]\n",
        "model_up.del_cache()\n",
        "\n",
        "# Show the output\n",
        "show_images(up_samples)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GLIDE_demo.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}